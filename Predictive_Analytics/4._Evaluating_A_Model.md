### 4. Evaluating a Model
The models formed throughout this project were AR, ARMA, ARIMA and SARIMA.
This sub-chapter will compare the models and explain the differences.

#### AR:
- Used a small amount of the data but still predicted in a fairly good way.
- Not elligible for tuning parameters as it only takes in training data.
- [Code](https://github.com/georgeottens/AppliedDataScience/blob/main/Python_Notebooks/AR_model_klant_69_werkelijke_aantallen_1_maand.ipynb)

##### What did the model contribute?
This was the first model I formed.
It was difficult because I had no idea were to start.
After I got a better idea of constructing such a model, the coding and programming went faster and I understood it better.
As expected, the model did not predict as wanted.
I expected it to be very poor, because the model itself is not extensive.
Because my programming skills were still at the developing stage, I did not think of applying different applications as cross-validation and rolling window.
Because the model did not predict as wanted by PostNL, I continued with the next, slightly more extensive model.

#### ARMA:
- Follows the same trend as the real values, but on a higher level.
- Assuming it misses the integrated value used in the ARIMA model.
- [Code](https://github.com/georgeottens/AppliedDataScience/blob/main/Python_Notebooks/ARMA_model_klant_69.ipynb)

##### What did the model contribute?
The most contributed factor by the model was the fact that I was able to code a bit faster and a bit more efficient.
Because the prediction was way out of the league of were the prediction should be, and because I still did not know how to apply certain validation strategies, once again I continued with the next model.

#### ARIMA:
- Was not trained with specific training data, but trained on the difference of the data. So the model has seen the predicted data before.
- [Code](https://github.com/georgeottens/AppliedDataScience/blob/main/Python_Notebooks/ARIMA_model_klant_69_YEET.ipynb)

##### What did the model contribute?
At the time of making this model I still did not know how to apply only certain data to predict the next.
The model had already seen the data that it needed to predict, so the prediction was compromised.
Because the hypothesis at the beginning of the project was that the most extensive Time Series model was needed for our project, I continued to the final kind of model I thought would be able to predict better then these previous models.


#### SARIMA:
- Was not trained with specific training data, but trained on the difference of the data. So the model has seen the predicted data before.
- [Code](https://github.com/georgeottens/AppliedDataScience/blob/main/Python_Notebooks/SARIMA_model_klant_69_YEET.ipynb)

As I learned more about the dividing in train, validation and test, it became clear to me that the SARIMA model was the best model to use.
Also I have seen that it takes in more parameters to alter which was necessary for our dataset predictions.

While this was the conclusion I drew, I also learned about applying different cross-validation techniques for better predictions.
- Applying SARIMA with difference in packages and without rolling window([Code](https://github.com/georgeottens/AppliedDataScience/blob/main/Python_Notebooks/SARIMA_model_klant_69_train_val_test_optimaliseren.ipynb)), resulted in a prediction that was okay, but could be enhanced using [rolling window](https://github.com/georgeottens/AppliedDataScience/blob/main/Python_Notebooks/SARIMA_model_klant_69_rolling_window_verschil.ipynb)
- Because difference in packages is not very clear, using [real values with rolling window](https://github.com/georgeottens/AppliedDataScience/blob/main/Python_Notebooks/SARIMA_model_klant_69_rolling_window_werkelijk.ipynb) resulted in a better view.

It was suggested to me to filter out the peaks of the dataset, but this did not enhance the prediction for the rest of the dataset. [Difference](https://github.com/georgeottens/AppliedDataScience/blob/main/Python_Notebooks/SARIMA_model_klant_69_rolling_window_verschil_pieken_filteren.ipynb)/[Real](https://github.com/georgeottens/AppliedDataScience/blob/main/Python_Notebooks/SARIMA_model_klant_69_rolling_window_werkelijk_pieken_filteren.ipynb)
As can be seen the filtered peaks with real values did not work but also was not expected to result in better predictions.

In the end of programming the predictions using the SARIMA model, an extra year of training was used.
Explained in previous sub-chapters, this was used to compair the SARIMA model to the Multivariate Linear Regression model where other groupmembers were working on.
The results 
### Conclusion:






























The models I formed were the AR, ARMA, ARIMA and SARIMA model.
From AR to SARIMA the model gets more and more extensive.

The AR model only takes trainingdata.
The ARMA model takes trainingdata, an AutoRegressive factor and a Moving Average factor.
The ARIMA model takes trainingdata, an AutoRegressive factor and an Integrated Moving Average factor where you can select how integrated a certain moving average factor is.
The SARIMA model takes trainingdata, an AutoRegressive factor and an Integrated Moving Average factor and all these factors with seasonal influences.

Because I delt with a dataset that was very extensive relating peaks and seasonal changes, my hypothesis was that the SARIMA model was the best model for making predictions.
