### 4. Evaluating a Model
The models formed throughout this project were AR, ARMA, ARIMA and SARIMA.
This sub-chapter will compare the models and explain the differences.

#### AR:
- Used a small amount of the data but still predicted in a fairly good way.
- Not elligible for tuning parameters as it only takes in training data.
- [Code](https://github.com/georgeottens/AppliedDataScience/blob/main/Python_Notebooks/AR_model_klant_69_werkelijke_aantallen_1_maand.ipynb)

#### ARMA:
- Follows the same trend as the real values, but on a higher level.
- Assuming it misses the integrated value used in the ARIMA model.
- [Code](https://github.com/georgeottens/AppliedDataScience/blob/main/Python_Notebooks/ARMA_model_klant_69.ipynb)

#### ARIMA:
- Was not trained with specific training data, but trained on the difference of the data. So the model has seen the predicted data before.
- [Code](https://github.com/georgeottens/AppliedDataScience/blob/main/Python_Notebooks/ARIMA_model_klant_69_YEET.ipynb)

#### SARIMA:
- Was not trained with specific training data, but trained on the difference of the data. So the model has seen the predicted data before.
- [Code](https://github.com/georgeottens/AppliedDataScience/blob/main/Python_Notebooks/SARIMA_model_klant_69_YEET.ipynb)
- As the dividing in train, validation and test sets took place, it became clear that the SARIMA model was the best model to use.
This is because it also takes in more parameters to alter. While this was the conclusion, different cross-validation techniques were applied for better predictions.
- Applying SARIMA with diffference in packages and without rolling window([Code](https://github.com/georgeottens/AppliedDataScience/blob/main/Python_Notebooks/SARIMA_model_klant_69_train_val_test_optimaliseren.ipynb)), resulted in a prediction that was okay, but could be enhanced using [rolling window](https://github.com/georgeottens/AppliedDataScience/blob/main/Python_Notebooks/SARIMA_model_klant_69_rolling_window_verschil.ipynb)
- Because difference in packages is not very clear, using [real values with rolling window](https://github.com/georgeottens/AppliedDataScience/blob/main/Python_Notebooks/SARIMA_model_klant_69_rolling_window_werkelijk.ipynb) resulted in a better view.
- It was suggested to filter out the peaks of the dataset, but this did not enhance the prediction for the rest of the dataset. [Difference](https://github.com/georgeottens/AppliedDataScience/blob/main/Python_Notebooks/SARIMA_model_klant_69_rolling_window_verschil_pieken_filteren.ipynb)/[Real](https://github.com/georgeottens/AppliedDataScience/blob/main/Python_Notebooks/SARIMA_model_klant_69_rolling_window_werkelijk_pieken_filteren.ipynb)
- As can be seen the filtered peaks with real values did not work but also was not expected to result in better predictions.

Conclusion:
- The AR model predicted well enough by having just a slight difference in predicted and real values, but it cannot be altered with parameters or subfactors.
- The ARMA model followed the same trend but at a higher level resulting in a too high prediction.
- The ARIMA model was not trained with the right training values, but also did not predict well enough with the data it was trained with.
- The SARIMA model itself could work well, but has a too big of a validation set to predict over.
Using Rolling Window dealt with this and predicted well enough over a whole year.
Using real values gives a better understanding on how well the model performed.
