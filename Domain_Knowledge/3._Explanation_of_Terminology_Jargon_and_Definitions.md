# 3. Explanation of Terminology, jargon and definitions

To understand the project one must have some knowledge about Data Science terminology, jargon and definitions.
The next few explanations of these 3 Data Science classifications were derived from [these](https://www.dataquest.io/blog/data-science-glossary/) two [links](https://www.springboard.com/blog/data-science-terms/#:~:text=At%20its%20essence%2C%20data%20science,%2C%20data%20mining%2C%20and%20programming.).

- **Python**

An object-oriented programming language often used in data science because users have developed an extensive array of tools applicable to the field. Python is free to use for commercial or personal projects, and it’s often commended for its learnability for programmers and non-programmers alike.

- **Machine Learning**

A process where a computer uses an algorithm to gain understanding about a set of data, then makes predictions based on its understanding.
There are many types of machine learning techniques; most are classified as either supervised or unsupervised techniques.

- **Overfitting**

Overfitting happens when a model considers too much information.
It’s like asking a person to read a sentence while looking at a page through a microscope.
The patterns that enable understanding get lost in the noise.

- **Underfitting**

Underfitting happens when you don’t offer a model enough information.
An example of underfitting would be asking someone to graph the change in temperature over a day and only giving them the high and low.
Instead of the smooth curve one might expect, you only have enough information to draw a straight line.

- **Regression**

Regression is another supervised machine learning problem.
It focuses on how a target value changes as other values within a data set change.
Regression problems generally deal with continuous variables, like how square footage and location affect the price of a house.

- **Training and Testing**

This is part of the machine learning workflow.
When making a predictive model, you first offer it a set of training data so it can build understanding.
Then you pass the model a test set, where it applies its understanding and tries to predict a target value.

- **Data Analysis**

This discipline is the little brother of data science.
Data analysis is focused more on answering questions about the present and the past.
It uses less complex statistics and generally tries to identify patterns that can improve an organization.

- **Data Engineering**

Data engineering is all about the back end.
These are the people that build systems to make it easy for data scientists to do their analysis.
In smaller teams, a data scientist may also be a data engineer.
In larger groups, engineers are able to focus solely on speeding up analysis and keeping a data well organized and easy to access.

- **Data Science**

Given the rapid expansion of the field, the definition of data science can be hard to nail down.
Basically, it’s the discipline of using data and advanced statistics to make predictions.
Data science is also focused on creating understanding among messy and disparate data.
The “what” a scientist is tackling will differ greatly by employer.

- **Data Visualization**

The art of communicating meaningful data visually.
This can involve infographics, traditional plots, or even full data dashboards.

- **Correlation**

Correlation is the measure of how much one set of values depends on another.
If values increase together, they are positively correlated.
If one values from one set increase as the other decreases, they are negatively correlated.
There is no correlation when a change in one set has nothing to do with a change in the other.

- **Mean (Average, Expected Value)**

A calculation that gives us a sense of a “typical” value for a group of numbers.
The mean is the sum of a list of values divided by the number of values in that list.
It can be deceiving used on its own, and in practice we use the mean with other statistical values to gain intuition about our data.

- **Median**

In a set of values listed in order, the median is whatever value is in the middle.
We often use the median along with the mean to judge if there are values that are unusually high or low in the set.
This is an early hint to explore outliers.

- **Time Series**

A time series is a set of data that’s ordered by when each data point ocurred.
Think of stock market prices over the course of a month, or the temperature throughout a day.

- **Residual (Error)**

The residual is a measure of how much a real value differs from some stastical value we calculated based on the set of data.
So given a prediction that it will be 20 degrees celcius at noon tomorrow, when noon hits and its only 18 degrees, we have an error of 2 degrees.
This is often used interchangably with the term “error,” even though, technically, error is a purely theoretical value.

- **Variance**

The variance of a set of values measures how spread out those values are.
Mathematically, it is the average difference between individual values and the mean for the set of values.
The square root of the variance for a set gives us the standard deviation, which is more intuitively useful.

- **Deep Learning**

Deep learning models use very large neural networks — called deep nets — to solve complex problems, such as facial recognition.
The layers in a model start with identifying very simple patterns and then build in complexity.
By the end the net (hopefully) has a nuanced understanding that can accurately classify or predict values.

- **Feature Engineering**

The process of taking knowledge we have as humans and translating it into a quantitative value that a computer can understand.
For example, we can translate our visual understanding of the image of a mug into a representation of pixel intensities.

- **Feature Selection**

The process of identifying what traits of a data set are going to be the most valuable when building a model.
It’s especially helpful with large data sets, as using fewer features will decrease the amount of time and complexity involved in training and testing a model.
The process begins with measuring how relevant each feature in a data set is for predicting your target variable.
You then choose a subset of features that will lead to a high-performance model.

- **Neural Networks**

A machine learning method that’s very loosely based on neural connections in the brain.
Neural networks are a system of connected nodes that are segmented into layers — input, output, and hidden layers.
The hidden layers (there can be many) are the heavy lifters used to make predictions.
Values from one layer are filtered by the connections to the next layer, until the final set of outputs is given and a prediction is made.
